# DistributedProcessWatcher
<p>Fully distributed asynchronous highly-available Process Monitoring library, specially useful when there are multiple processes that need to be running continuously or data loss might occur. For example, consider a scenario where data is appended to a log file, and we want to store landing data to analyze later with some tool from hadoop ecosystem.</p>

<p>We'd need at least a program monitoring changes to the file and later send this changes to hadoop. Flume would be an useful tool since with an Exec source, we could tail -F the file, send them through a memory channel and use a hdfs sink to persist data. If for some reason (and this is just an example) flume dies, we're gonna stop getting data from the log. With this Process Watcher, we can easily achieve automatic re-deploy of flume and keep getting data from the log.</p><br/>

<b>How does it work?</b>
<p>It comprises three main components: time masters (TM), master watchers (parent: MW and child: CMW) and process wrappers (PW). There's some sort of hierarchy amongst these components, in the sense that MWs rely on TMs to work, CMWs rely on MWs for coordination and on PWs for spawning processes. In order to achieve coordination and synchronization, all components depend on <a href="https://zookeeper.apache.org/">Apache ZooKeeper</a>. TMs and MWs have an <b>autonomous Active Master (ATM or AMW) election</b> feature, that is, they compete against each other to see which TM or MW becomes the next ATM or AMW. The rest of the masters are inactive masters (ITM or IMW). Now, ITMs and IMWs have to monitor that the current ATM or AMW is online and that it is doing its job. If for any reason, an ATM or AMW stops responding (that is, reporting its health status to its peers inactive masters), inactive masters will start a new competition after following a <a href="">Safety Deletion Policy</a>, to see which ITM or IMW is the next active master.</p>
<p><i>Components explained</i></p>
<ul>
  <li><b>Time Masters:</b> Top level masters requesting time from NTP servers and providing them to lower level components. Competition for active mastership consists of creating a time znode, and the instance that manages to create this instance becomes the ATM. When deploying TMs, the user must indicate two znodes. The first one called <i>time znode</i> (AKA time masters' keep alive znode), where ATM pushes heartbeat according to the time interval provided, and ITMs verify that it is pushing heartbeats within specified threshold. The other znode is called <i>znode for time listeners</i>, used by lower level masters to get time. For high availability, deploy TMs in diferent servers.</li>
  <li><b>Master Watchers:</b> A MW can be one of two types, parent (normally referred to as MW only) or child MW. The reason for having two types of masters is to allow distributed components, that is, a MW can have many CMWs and they can be deployed in different nodes, and CMWs are the one spawning the processes that must remain running most of the time, and must be restarted if anything brings them down. Unlike MWs, CMWs do not have an active CMW election feature so in order to achieve high availability, more than one MW must be deployed and a CMW linked to each of the MWs must be deployed. For example, say you have 4 MWs (mw1, mw2, mw3, mw4), and 2 different processes that must be monitored, you would then deploy 8 CMWs like this (cmw1 linked to mw1; cmw1 linked to mw2; cmw1 linked to mw3; cmw1 linked to mw4; cmw2 linked to mw1; cmw2 linked to mw2; cmw2 linked to mw3; cmw2 linked to mw4). That way, if mw1 is elected as active master, cmw1 and cmw2 linked to mw1 trigger; if for some reason mw1 dies, and mw4 becomes the new active master, cmw1 and cmw2 linked to mw4 will trigger. Now, even though there are 8 cmws, since they all deploy the same process, all of them must have the same id, but the MW they are linked to, will vary in each copy. 
<br/>
MWs do have the autonomous election feature, and if IMWs detect that current active MW is failing to push heartbeat updates, they compete against each other to have a new active MW. To see which MW is the next AMW, all MWs try to create the <b>znode for time listeners</b>. The MW that creates this znode first, becomes the AMW, while the rest remain as IWMs. After receiving the first time tick, AMW creates the znode <b>masters' keep alive znode</b>, where it will push heartbeat updates and IMWs will use it to verify AMW's status, and the <b>process observed znode</b>, which will cause CMWs to spawn processes that must be monitored. Every time the ATM pushes a new time tick, after the initialization is done, the AMW will request CMWs to report their health statuses, and as it gets a report from its children, it will append it to a queue. When the queue is full, the AMW will clear the queue and post an update to master's keep alive znode. If some CMWs fail to report their health statuses, they will be appended to a failing children list, after receiving a warning, when they become critical, AMW will kill itself and all of its children, condition which will be detected by IMWs and after following the <a href="">Safety Deletion Policy</a>, they will compete against each other. The whole process repeats as many times as there are failing AMW and available IMWs.</li>
<li><b>Child Master Watcher:</b> </li>
